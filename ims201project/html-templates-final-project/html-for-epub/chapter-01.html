<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<!-- HTML template for IMS201 e-book project -->
<!-- Elias Tzoc - November 2014 -->

<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<title>Chapter 1: Making Digital Writing Assessment Fair for Diverse Writers</title>
<link rel="stylesheet" type="text/css" href="style-for-epub.css" />
</head>

<body>

<h1>Chapter 1: Making Digital Writing Assessment Fair for Diverse Writers</h1>
<i>Mya Poe</i>&nbsp;&nbsp;&nbsp;:::&nbsp;&nbsp;&nbsp;
<a href="mailto:UniqueID@miamioh.edu?Subject=IMS201%20ePUB%20Project">UniqueID@miamioh.edu</a>
<br /><br /><br />

<h3>ABSTRACT</h3>
<p>
The move to large-scale digital writing assessment brings about considerations related to validity, reliability, and fairness. In this chapter, I discuss the issue of fairness in digital writing assessment and suggest possible ways that considerations of fairness may be brought into the design and use of digital writing assessment. Although there are many sources where we may look for guidance on fairness in assessing digital writing, a useful starting point is the <em>Standards for Educational and Psychological Testing.</em> The Standards offer agreed-upon principles for the design, administration, and interpretation of educational and psychological tests and have been adopted by writing assessment scholars. In this chapter, I explain several of the 12 fairness guidelines offered in the Standards and explain their significance for digital writing assessment. In the end, I argue that digital writing assessment must go beyond the inductive design of rubrics to include theoretically informed fairness inquiries to ensure that we are working to make digital writing assessment equitable for all students. </p>


<h3>INTRODUCTION</h3>
<p>
Studies by the <em>Pew Internet and American Life Project</em> (Lenhart, Arafeh, Smith, &amp; MacGill, 2008) as well as projects such as the Stanford Study of Writing (2008) remind us that digital composing is omnipresent in the lives of many young people today. As a consequence, composition studies scholars—well aware of the need to bridge digital literacies and academic literacies—have explored numerous ways to integrate multimodal, networked, and other digital technologies in writing classrooms (see, for instance, Kimme Hea, 2009; Selfe, 2007; Wysocki, Johnson-Eilola, Selfe, &amp; Sirc, 2004). While digital technologies have undoubtedly brought enormous potential in expanding the teaching of writing (NCTE, 2008; <em>New London Group</em>, 1996; Selber, 2004; Selfe, 1999), how to assess digital writing has raised new questions, such as how digital media change the possibilities of composing and, thus, the construct of writing (Yancey, 2004); how readers interact with digital texts (Takayoshi, 1996); and how we can link digital writing assessment to student learning (Huot, 1996a). Digital writing assessment also raises issues related to fairness, especially in assessing the digital writing of diverse groups of students. </p>

<p>
Issues of fairness are particularly important in large-scale digital writing assessment, assessments in which large numbers of students are assessed across departments, institutions, states, or countries often for diagnostic or proficiency purposes (Whithaus, 2005). Large-scale assessments, like placement or proficiency testing, often dictate student ability to access educational resources or advance through the higher educational system. Moreover, as Michael Neal (2011) pointed out, states are moving to include digital writing in state-level testing programs:
</p>

<blockquote>
<p>
Digital technologies are embedded within the fabric of many composition outcomes and state writing standards that steer instruction and assessment of student writing. In my home state of Florida, the Sunshine State Standards affirm that a student “selects and use a variety of electronic media, such as the Internet, information services, and desk-top publishing software programs, to create, revise, retrieve, and verify information.” (9–12 Grade Language Arts, LA.B.2.4.4) (p. 4)
</p>  
</blockquote>

<h3>WRITING ASSESSMENT AND TECHNOLOGIES</h3>

<p>
In <em>Writing Assessment and the Revolution in Digital Texts and Technologies,</em> Michael Neal (2011) traced two themes: (1) writing assessment as technology, and (2) writing assessment with technology. Writing assessment as technology has been a popular framework in assessment scholarship for two decades. Composition scholars like Brian Huot (1996b), Peggy O’Neill (1998), and Asao Inoue (2009) draw from George Madaus (1993) and F. Allan Hanson (1993) in framing assessment as a technology. Madaus explained that testing fits even “very simple definitions of technology—the simplest being something put together for a purpose, to satisfy a pressing and immediate need, or to solve a problem” (pp. 12–13). Unlike other technologies, however, accessible to the public, assessment technologies may be widely applied but not widely transparent to the public because of the testing community’s closed membership and specialized vocabulary. Although the testing community purports objectivity, Madaus argued that tests are “culturally constructed realities” (p. 223). Those realities, F. Allan Hanson (1993) wrote, are the realities of test designers—the tests “act to transform, mold, and even to create what they supposedly measure” and the testing situations “entail the application of power over the subjects of tests” (p. 52). O’Neill wrote that writing assessment, like other assessment technologies, is a “technical craft [that] is embedded in sociotechnical systems” and “shares the same power and biases inherent in all technology” (p. 7). </p>

<h3>VALIDITY, RELIABILITY, AND FAIRNESS</h3>
<p>
In composition studies today, most writing assessment is based on a contextualized model of assessment, in which assessments are designed, delivered, and interpreted locally (see Condon, 2011, for a review). Such “shared evaluation” or “organic” procedures have helped define writing assessment in a theoretically informed manner and in ways meaningful to writing teachers because they use teacher expertise to guide assessment decisions (Broad et al., 2009; Royer &amp; Gilles, 2003).
</p>

<p>
Contextualized views of writing assessment are consistent with newer conceptions of validity as found in the measurement literature (Moss, 1992). Validity—“the degree to which evidence and theory support the interpretation of test scores entailed by the proposed uses of tests”—is “the most fundamental consideration” in developing, designing, and using assessments (AERA/APA/NCME, 1999, p. 9). Contemporary validity theory is indebted to the work of the late ETS senior researcher Samuel Messick (1990), who argued that validity was not a series of individual empirical components, but, rather, a unified concept based on empirical evidence and theoretical rationales, which “support the adequacy and appropriateness of interpretations and actions based on test scores or other modes of assessment” (p. 1). Messick (1994) wrote that in moving from a task-based conception of assessment to a construct-based conception of assessment, we “begin by asking what complex of knowledge, skills, or other attributes should be assessed, presumably because they are tied to explicit or implicit objectives of instruction or are otherwise valued by society” (p. 16). We then inquire what behaviors or performances demonstrate that construct.
</p>

<h3>REFERENCES </h3>
<p>
Baldwin, Doug. (2012). Fundamental challenges in developing and scoring constructed-response assessments. In Norbert Elliot &amp; Les Perelman (Eds.), <em>Writing assessment in the 21st century: Essays in honor of Edward M. White</em> (pp.327–343).Cresskill, NJ: Hampton Press. </p>

<p>
Compaine, Benjamin M. (2001). <em>The digital divide: Facing a crisis or creating a myth.</em> Boston, MA: MIT Press.
</p>

<p>
Johnson, David, &amp; Van Brackle, Lewis. (2011). Linguistic discrimination in writing assessment: How raters react to African American “errors,” ESL errors, and standard English errors on a state-mandated writing exam. <em>Assessing Writing</em>, 17 (1), 35–54. </p>

<p>
Zdenek, Sean. (2009). Accessible podcasting: College students on the margins in the new media classroom. Computers and Composition Online. Retrieved from <a href="http://seanzdenek.com/article-accessible-podcasting/">http://seanzdenek.com/article-accessible-podcasting/</a> 
</p>

</body>
</html>
